{
  
    
        "post0": {
            "title": "Finding Similar Time-Series",
            "content": "Ever since watching this presentation about quasi-experiments at Netflix I&#39;ve been trying to understand more about how to operationalize a similar model of testing. In my opinion, I&#39;ve found multi-channel attribution to be difficult because there are only so many insights I can believe. For example, finding a campaign that consistently results in $0 of revenue is probably accurate; when I&#39;ve dug into issues like that further, et voilà, I&#39;ve find ad fraud. On the other hand, an attribution tool that promises insights into the return from paid social media traffic (which I&#39;ve typically seen to be &gt;80% smartphone traffic) seems dubious. While quasi-experiments aren&#39;t perfect, I find the premise more promising than tying together a bunch of cookies -- and much, much better than utilizing the sketchy companies that have resolved cookies/email addresses/devices to a person. There are also some really cool use cases for this type of approach, like Airbnb experimenting with UX/SEO optimization. . While there&#39;s a lot of details that go into setting up infrastructure to support near-experimental design with marketing platforms, one of the starting points is finding which geographic regions are most similar so they can serve as control/test groups. One method that comes up oftern is dynamic time warping, a great example of which is the MarketMatching R package : . The MarketMatching package implements the workflow described above by essentially providing an easy-to-use &quot;wrapper&quot; for the dtw and CausalImpact. The function best_matches() finds the best control markets for each market by looping through all viable candidates in a parallel fashion and then ranking by distance and/or correlation. The resulting output object can then be passed to the inference() function which then analyzes the causal impact of an event using the pre-screened control markets. . If you&#39;d like to learn more about dynamic time warping and classification problems for time-series, see Aileen Nielsen&#39;s book Practical Time Series Analysis. Now on to the code. Of course, to make things more difficult for myself I want to try this out in Python... . %matplotlib inline import numpy as np from numpy.random import randn import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm from statsmodels.tsa.arima_process import arma_generate_sample np.random.seed(12345) . arparams = np.array([.75, -.25]) maparams = np.array([.65, .35]) ar = np.r_[1, -arparams] # add zero-lag and negate ma = np.r_[1, maparams] # add zero-lag y = sm.tsa.arma_generate_sample(ar, ma, nsample=250, distrvs=randn) # model = sm.tsa.ARMA(y, (2, 2)).fit(trend=&#39;nc&#39;, disp=0) # model.params fig, ax = plt.subplots() ax.plot(y, label=&quot;Data&quot;) . [&lt;matplotlib.lines.Line2D at 0x1c1b956828&gt;] . fig, ax = plt.subplots() ax.plot(randn(250).cumsum(), &#39;k&#39;, label=&#39;one&#39;) . [&lt;matplotlib.lines.Line2D at 0x1c1b62cda0&gt;] . from scipy.spatial.distance import euclidean from fastdtw import fastdtw .",
            "url": "http://blog.measureallthethin.gs/viz/jupyter/2020/05/07/time-series-and-time-warp.html",
            "relUrl": "/viz/jupyter/2020/05/07/time-series-and-time-warp.html",
            "date": " • May 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Plotting Fun With Matlotlib (and some Seaborn assistance)",
            "content": "Numerous times I&#39;ve found myself in a situation where data is not yet in a datawarehouse or available in a visualization tool (such as Looker), but nonetheless charts were expected for scheduled reports. This is a gallery of some of the charts I&#39;ve made. It&#39;s important to note that Chris Moffit&#39;s post on Effectively Using Matplotlib was instrumental to finally grokking how to interact with that charting library. . %matplotlib inline import numpy as np from numpy.random import randn import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set_style(&#39;whitegrid&#39;) sns.set_context(&quot;talk&quot;) np.random.seed(12345) . Create Some Fake Data . Most of the charts I deal with in Marketing Analytics / Business Operations have been based on timeseries data. In prepping for this post, I found simulating timeseries, especially the patterns I see frequently in marketing analytics, not as straightforward. Luckily a few different people on Twitter were helpful, especially this suggestion from Shahar Shani-Kadmiel: . pic.twitter.com/LYqJSLsZyx . &mdash; Shahar Shani-Kadmiel (@shaharkadmiel) May 5, 2020 . # set the time range time = pd.date_range( start = &#39;2018-01-01&#39;, end = &#39;2019-12-31&#39;, freq = &#39;D&#39; ) # leverage sine waves to create daily data with weekly patterns and a trend that shows growth over time amp = 1000 freq = 52 / 365 data = amp * np.sin(freq * 2 * np.pi * np.arange(time.size)) noise = 2 * np.random.rand(time.size) - 1 data += 0.2 * data.ptp() * noise trend = np.power(np.arange(time.size),1.25) data += trend data += 50000 data = np.around(data,decimals=0).astype(int) . def generate_series(time,amp,freq,power,vertical_shift): data = amp * np.sin(freq * 2 * np.pi * np.arange(time.size)) noise = 2 * np.random.rand(time.size) - 1 data += 0.2 * data.ptp() * noise trend = np.power(np.arange(time.size),power) data += trend data += vertical_shift data = np.around(data,decimals=0).astype(int) return data . date_range = pd.date_range(start = &#39;2018-01-01&#39;, end = &#39;2019-12-31&#39;,freq = &#39;D&#39;) . web_traffic = generate_series(date_range,10000,(52 / 365),1.45,50000) fig, ax = plt.subplots(figsize=(12,8)) ax.plot(date_range,web_traffic) . [&lt;matplotlib.lines.Line2D at 0x1c367906a0&gt;] . acct_create = generate_series(date_range,1000,(52 / 365),0.95,1000) fig, ax = plt.subplots(figsize=(12,8)) ax.plot(date_range,acct_create) . [&lt;matplotlib.lines.Line2D at 0x1c36d462b0&gt;] . traffic = pd.DataFrame(web_traffic,index=time,columns=[&#39;website_traffic&#39;]) traffic.index.name = &#39;date&#39; . accts = pd.DataFrame(acct_create,index=time,columns=[&#39;accounts_created&#39;]) accts.index.name = &#39;date&#39; . df = pd.concat([traffic,accts],axis=1) df = df.reset_index() df.head() . date website_traffic accounts_created . 0 2018-01-01 | 52765 | 976 | . 1 2018-01-02 | 56745 | 2065 | . 2 2018-01-03 | 61969 | 1675 | . 3 2018-01-04 | 54372 | 1212 | . 4 2018-01-05 | 42823 | 474 | . df.groupby(pd.Grouper(key=&#39;date&#39;,freq=&#39;MS&#39;)).sum().plot(subplots=True, layout=(1,2), figsize=(14, 6), sharey=False, ylim = 0); . df.groupby(pd.Grouper(key=&#39;date&#39;,freq=&#39;MS&#39;)).sum().assign(signup_rate = lambda x: x[&#39;accounts_created&#39;] / x[&#39;website_traffic&#39;]).loc[:,&#39;signup_rate&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c37b0a358&gt; .",
            "url": "http://blog.measureallthethin.gs/viz/jupyter/2020/05/05/plotting-with-matplotlib.html",
            "relUrl": "/viz/jupyter/2020/05/05/plotting-with-matplotlib.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Connect Python and Pandas To Redshift",
            "content": "There are many situations where you might like to get data from a database into a pandas dataframe. The simplest way to is to initialize a dataframe via the pandas read_sql_query method. The ORM of choice for pandas is SQLAlchemy. By installing a couple more packages, you can query Redshift data and read that into a dataframe with just a few lines of of Python code. . The Setup . This post assumes you have a number of Python packages already installed, such as pandas, numpy, sqlalchemy and iPython or Jupyter Lab. I&#39;ve used the Python distribution Anacoda, which can be downloaded here. It comes packed with many of the libraries you might need for data analysis. . The Dialects documentation for SQLAlchemy mentions that Redshift is supported through another Python package, which itself depends on a PostgreSQL driver. So, the additional packages needed for connecting to Redshift are redshift-sqlalchemy and psycopg2. If you already have Anaconda, you can install psycopg2 quickly using conda. For getting redshift-sqlalchemy installed, here are some docs on using conda &amp; pip together. That being said, the process of downloading the packages looked like this in my terminal: . conda install psycopg2 cd Downloads/redshift-sqlalchemy-0.4.1 python setup.py install . Of course, I did this in my default environment. Everything seems to work, but the Anaconda docs are very useful if you have multiple Python environments on your machine. . On To The Data . Start iPython in your terminal (or Notebook if you prefer). Note that the connection string follows the pattern &#39;flavor+DBAPI://username:password@host:port/database&#39;. Make sure to change the connection string below to your own! . import pandas as pd import numpy as np from sqlalchemy import create_engine # what is the database equivalent of &#39;hello world&#39; anyways? test_query = &quot;&quot;&quot; SELECT COUNT(*) FROM datawarehouse.users u WHERE u.date BETWEEN &#39;2015-01-01&#39; AND &#39;2015-05-31&#39; &quot;&quot;&quot; red_engine = create_engine(&#39;redshift+psycopg2://username:password@your.redshift.host.123456abcdef.us-east-1.redshift.amazonaws.com:port/database&#39;) test = pd.read_sql_query(test_query,red_engine) . If we look at the &quot;test&quot; object I see: . In [5]: test Out[5]: count 0 178545 . Of course, you&#39;ll need to modify the query string to match the settings for your own warehouse to test this out. . Update/Issues . It&#39;s been a while since I had to use this type of connection, so things may have changed since this was originally in June of 2015. Specifically, once my company required SSL connections, the setup above went south and I got errors like: . OperationalError: (OperationalError) sslmode value &quot;require&quot; invalid when SSL support is not compiled in None None . Samantha Zeitlin responded to my request for help with a super-thorough post to help debug the issue, which had to do with fixing symlinks. Unfortunately the Google Group where that response was posted is now defunct :weary: . Miraculously I happen to have the last post to that thread: . For posterity, a workaround that worked for me without having to rely on symlinks:&gt; &gt; As others have suggested, install with pip:&gt; $ `conda uninstall psycopg2`&gt; $ pip install psycopg2 . Python will throw an error when trying to import psycopg2, so run this:&gt; $ `export DYLD_FALLBACK_LIBRARY_PATH=$HOME/anaconda/lib/:$DYLD_FALLBACK_LIBRARY_PATH`&gt; &gt; Credit:http://stackoverflow.com/questions/27264574/import-psycopg2-library-not-loaded-libssl-1-0-0-dylib . If you have any issues or fixes, I&#39;d love to hear about them; you can reach out to me on Twitter. .",
            "url": "http://blog.measureallthethin.gs/redshift/pandas/2020/05/04/connecting-python-and-pandas-to-redshift.html",
            "relUrl": "/redshift/pandas/2020/05/04/connecting-python-and-pandas-to-redshift.html",
            "date": " • May 4, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Charting a Marketing Acquisition Funnel",
            "content": "%matplotlib inline import pandas as pd from collections import OrderedDict import matplotlib.pyplot as plt import matplotlib.patches as patches import seaborn as sns sns.set_style(&#39;whitegrid&#39;) . # start with some dummy data df = pd.DataFrame({ &#39;date&#39;:[&#39;2019-08-01&#39;,&#39;2019-09-01&#39;], &#39;website_traffic&#39;:[530771,558652], &#39;new_accounts&#39;:[15720,15900], &#39;account_verify&#39;:[12670,13884], &#39;product_activated&#39;:[10812,12909], &#39;paid_conversion&#39;:[654,908], &#39;some_other_metric&#39;:[13270,13678] }) df . date website_traffic new_accounts account_verify product_activated paid_conversion some_other_metric . 0 2019-08-01 | 530771 | 15720 | 12670 | 10812 | 654 | 13270 | . 1 2019-09-01 | 558652 | 15900 | 13884 | 12909 | 908 | 13678 | . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;],format=&#39;%Y-%m-%d&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2 entries, 0 to 1 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 date 2 non-null datetime64[ns] 1 website_traffic 2 non-null int64 2 new_accounts 2 non-null int64 3 account_verify 2 non-null int64 4 product_activated 2 non-null int64 5 paid_conversion 2 non-null int64 6 some_other_metric 2 non-null int64 dtypes: datetime64[ns](1), int64(6) memory usage: 240.0 bytes . # define only the KPIs we want to see (since there are cases where other metrics are present in the dataframe) all_kpis = [&#39;website_traffic&#39;,&#39;new_accounts&#39;,&#39;account_verify&#39;,&#39;product_activated&#39;,&#39;paid_conversion&#39;] . all_names = tuple([x.replace(&#39;_&#39;,&#39; n&#39;).title() for x in all_kpis]) . def get_it_in_order(df,mask,month): &quot;&quot;&quot; Biggest assumption: there is a &#39;date&#39; column (not index!) in the dataframe that is *literally named* &#39;date&#39; Accepts entire dataframe Supply a column mask Supply a list of column names &quot;&quot;&quot; kpis = OrderedDict() frame_2_dict = df.set_index(&#39;date&#39;).loc[month,mask].to_dict() for i in mask: kpis[i] = frame_2_dict[i] return kpis . def add_arrow(x_adjust,y_adjust,ratio): bbox_props = dict(boxstyle=&quot;rarrow,pad=0.5&quot;, facecolor=&quot;grey&quot;, edgecolor=&quot;grey&quot;, alpha=0.75, lw=1) return plt.text(x_adjust,y_adjust,&#39;{0:.1f}%&#39;.format(ratio * 100), verticalalignment=&#39;center&#39;, horizontalalignment=&#39;left&#39;, fontsize=16, weight=&#39;black&#39;, color=&#39;white&#39;, bbox=bbox_props) . def plot_acquisition_funnel(df, month, names, metrics): data = get_it_in_order(df, metrics, month) count = range(len(data)) fig, ax = plt.subplots(figsize=(10, 6)) rects = ax.bar(count, data.values(), width=.5, align=&#39;center&#39;, color=sns.color_palette(&quot;Blues&quot;, len(data))) plt.xticks(count, names) plt.ylim(bottom=0, top=data[metrics[1]]*2) plt.title(&#39;{0} Acquisition Funnel&#39;.format(month), fontsize=16) plt.tick_params(labelsize=16) add_arrow(0.27, data[metrics[3]]/1.7, data[metrics[1]]/data[metrics[0]]) add_arrow(1.27, data[metrics[3]]/1.7, data[metrics[2]]/data[metrics[1]]) add_arrow(2.27, data[metrics[3]]/1.7, data[metrics[3]]/data[metrics[2]]) add_arrow(3.27, data[metrics[3]]/1.7, data[metrics[4]]/data[metrics[3]]) for rect, val in zip(rects,data.values()): height = rect.get_height() if height &gt; 250: label_color = &#39;grey&#39; else: label_color = &#39;white&#39; ax.text(rect.get_x() + rect.get_width()/2,250,&#39;{:,}&#39;.format(val),ha=&#39;center&#39;, va=&#39;bottom&#39;,fontsize=12,weight=&#39;bold&#39;,color=label_color) plt.grid(False) plt.show() . plot_acquisition_funnel(df,&#39;2019-08-01&#39;,all_names,all_kpis) .",
            "url": "http://blog.measureallthethin.gs/viz/jupyter/2020/05/02/charting-a-marketing-funnel.html",
            "relUrl": "/viz/jupyter/2020/05/02/charting-a-marketing-funnel.html",
            "date": " • May 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "http://blog.measureallthethin.gs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.measureallthethin.gs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
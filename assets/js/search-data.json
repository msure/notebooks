{
  
    
        "post0": {
            "title": "Access Search Console Data via Python",
            "content": "If this is your first time accessing data from a Google API, there&#39;s a lot of setup to be done: . Create a project in Google&#39;s Developer Console | Enable the API(s) you want to use | Create the consent screen | Create and download the OAuth credentials | Run the script | In this tutorial I&#39;ll assume you&#39;re running the code from your own computer. . Lots of Clicking First . There&#39;s a lot of steps to get done before actually getting to the code. Since I&#39;ve found myself having to do this every other year, I figured it&#39;d be good to leave step-by-step notes (screenshots included!). . First, you need to login to Developer Console and create a project: . . If you haven&#39;t already, create a new project: . . Name the project: . . You&#39;ll need to find the specific API you want to enable. I&#39;ve gotten lost trying to find which button will take you to the API library, so here&#39;s the link instead. When you&#39;ve found the API you&#39;re looking for, click the Enable button. While you&#39;re at it, you may want to search for and enable others(such as the Google Analytics Reporting API or the Youtube Reporting API) since you can add multiple APIs, or scopes, to one credential: . . Since I&#39;ll be demonstrating an Oauth verification flow, you&#39;ll need to create a consent screen: . . There&#39;s a dozen fields or so, but only one field in particular matters: the application name. Give it some generic name (but make sure not to include anything related to Google in the name field). You can scoll down to the bottom of the screen and click the Save button. . . Almost there. Create the credentials; I&#39;ve selected the OAuth option. . . Again, create a generic name, this time for the credentials themselves. One really, really important note: when I use Jupyter Lab on my Macbook Pro, when it starts up I get a notice that port 8888 is already in use, so instead it moves to port 8889...you&#39;ll need to specifiy an unused port here in the &quot;Authorized redirect URIs&quot; and remember the port because you&#39;ll use it again in the code below. In my case, port 8080 works: . . Now download the client secrets, and rename to client_secrets.json and move the file to the folder you&#39;ll access in the code. . . That marks the end of clicking around in Developer Console. However, note that the first time you run this script you&#39;ll have to authorize the script to access your data. Make I&#39;ll be using the account for my blog below. . There&#39;s a whole suite of Google libraries you&#39;ll need to install to make sure the script runs: . Google&#39;s Python API client | Google Auth | Google&#39;s OAuth integration with the Google Auth library | . Finally, I&#39;ve found the Progress library a fun way to keep track of how far the script has progressed. . import os import json import time import glob import pickle import random import pandas as pd from pandas import Series, DataFrame from progress.bar import IncrementalBar # these are just for authenticating import google.oauth2.credentials from google_auth_oauthlib.flow import Flow from google_auth_oauthlib.flow import InstalledAppFlow from google.auth.transport.requests import Request # this actually helps access the API from apiclient.discovery import build . PROPERTY_URI = &#39;http://measureallthethin.gs/&#39; OUTPUT_PATH = &#39;../output/search_console_data/&#39; SECRETS_PATH = &#39;../secrets/&#39; CLIENT_SECRETS = SECRETS_PATH + &#39;client_secrets.json&#39; # notice you can add multiple apis to the credential created earlier :-) SCOPES = [&#39;https://www.googleapis.com/auth/webmasters.readonly&#39;,&#39;https://www.googleapis.com/auth/analytics.readonly&#39;] . Probably the most painful part of interfacing with Google&#39;s APIs is the fact that every two years the process changes. I spent a whole day and a half trying to figure out how to use the new Google Auth library since some of the older libraries have been deprecated. I finally found a great example from the Google Docs team here that shows how to easily authenticate from your notebook or command line. . def prepare_credentials(): creds = None # the file token.pickle stores the user&#39;s access and refresh tokens, and is # created automatically when the authorization flow completes for the first # time. if os.path.exists(SECRETS_PATH + &#39;token.pickle&#39;): with open(SECRETS_PATH + &#39;token.pickle&#39;, &#39;rb&#39;) as token: creds = pickle.load(token) # if there are no (valid) credentials available, let the user log in if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) else: flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS, SCOPES) creds = flow.run_local_server(port=8080) # remember that port you put into the credentials? use that here in port param # save the credentials for the next run with open(SECRETS_PATH + &#39;token.pickle&#39;, &#39;wb&#39;) as token: pickle.dump(creds, token) return creds . As shown above, multiple APIs can be accessed from one project/set of credentials. I access the Google Analytics API frequently, so below is a quick example showing how to be ready for accessing different endpoints. . def initialize_service(service=&#39;batch&#39;): &quot;&quot;&quot; Returns a connection to the Google Analytics v4 API https://developers.google.com/analytics/devguides/reporting/core/v4/rest/ --OR-- Returns the Webmaster Service Object https://developers.google.com/webmaster-tools/search-console-api-original/v3/searchanalytics/query &quot;&quot;&quot; # make sure your authorized to access API first credentials = prepare_credentials() # build and return the service object if service == &#39;batch&#39;: return build(&#39;analyticsreporting&#39;, &#39;v4&#39;, credentials = credentials, discoveryServiceUrl=&#39;https://analyticsreporting.googleapis.com/$discovery/rest?version=v4&#39;) elif service == &#39;user_activty&#39;: return build(&#39;analyticsreporting&#39;, &#39;v4&#39;, credentials = credentials, discoveryServiceUrl=&#39;https://analyticsreporting.googleapis.com/v4/userActivity:search&#39;) elif service == &#39;webmaster&#39;: return build(&#39;webmasters&#39;, &#39;v3&#39;, credentials = credentials) else: raise Exception(&#39;Please select an endpoint!&#39;) . Typically I do not save the data to a variable but instead save it to disk so I can explore further in a notebook: . # save JSON of each day&#39;s/query&#39;s results def file_to_save(start,end,file_info): return OUTPUT_PATH + &#39;{0}_{1}_{2}.json&#39;.format(start,end,&#39;_&#39;.join(file_info)) . Google loves, loves returning nested JSON, which can be difficult to deal with in Pandas; this function makes it a bit easier to manage. . # &quot;flatten&quot; response a bit so it&#39;s easier to convert into a dataframe def tidy_the_nest(dims,results): for item in results: for row in item[&#39;rows&#39;]: row.update(dict(zip(dims,row[&#39;keys&#39;]))) return results . Ok, now the for a monster of a function. I&#39;ve tried commenting inline as much as possible the clarify the pieces, but overall the gist is that the request sent to Google needs to account for 1) the dimensions you want in the data (such as date, search keyword, etc), and 2) whether or not you&#39; . # accepts a list of dimensions by which to query the webmaster api, start and end dates, a collection of data, and integer to increment def execute_request(dims,start,end,results=None,i=0): # saw that get_search_data([&quot;date&quot;,&quot;query&quot;],date_range) was only returning 5k rows... # turns out when adding in keywords that is the max rows search console will return # max_rows = 5000 max_rows = 25000 if results is None: results = [] # create a dict of what exactly we want to query # https://developers.google.com/webmaster-tools/search-console-api-original/v3/searchanalytics/query payload = { &quot;startDate&quot;: start, # [Required] Start date of the requested date range, in YYYY-MM-DD format, in PST time (UTC - 8:00) &quot;endDate&quot;: end, # [Required] End date of the requested date range, in YYYY-MM-DD format, in PST time (UTC - 8:00) &quot;dimensions&quot;: dims, # &quot;country&quot;, &quot;device&quot;, &quot;page&quot;, &quot;query&quot;, &quot;searchAppearance&quot;: # https://developers.google.com/webmaster-tools/search-console-api-original/v3/how-tos/all-your-data#overview &quot;rowLimit&quot;: max_rows, # [Optional; Valid range is 1–25,000; Default is 1,000] The maximum number of rows to return. To page through results, use the startRow offset. &quot;startRow&quot;: i * max_rows # [Optional; Default is 0] Zero-based index of the first row in the response. Must be a non-negative number. If startRow exceeds the number of results for the query, the response will be a successful response with zero rows. } # now execute the query and save response data = service.searchanalytics().query(siteUrl=PROPERTY_URI, body=payload).execute() # max rows is 25000, so need to make sure we didn&#39;t leave any data behind if &#39;rows&#39; in data: results.append(data) num_of_rows = len(data[&#39;rows&#39;]) ### print(&#39;Aggregation {0}&#39;.format(data[&#39;responseAggregationType&#39;])) ### print(json.dumps(data[&#39;rows&#39;][0:2], sort_keys=True,indent=4, separators=(&#39;,&#39;, &#39;: &#39;))) else: num_of_rows = 0 ### print(&quot;Grabbed {0} rows for loop {1}&quot;.format(num_of_rows,str(i+1))) # get all the data by essentially paginating through results until none are left # https://developers.google.com/webmaster-tools/search-console-api-original/v3/how-tos/all-your-data # important to note this from the documentation: # Why do I lose data when asking for more detail? # When you group by page and/or query, our system may drop some data in order to be able to calculate results in a reasonable time using a reasonable amount of computing resources. # more about data can be found here https://support.google.com/webmasters/answer/6155685 if num_of_rows == max_rows: time.sleep(1 + random.random()) i += 1 return execute_request(dims,start,end,results=results,i=i) else: tidy_the_nest(dims,results) # save the json response because error handling not yet in place with open(file_to_save(start,end,dims), &#39;w&#39;) as fp: json.dump(results, fp) . Below is essentially a for loop that gets each day of data from a date range and helps keep track of your progress. . def get_search_data(dims,dates,n): &quot;&quot;&quot; dims: list of dimensions to include in query dates: date range of type pandas.core.indexes.datetimes.DatetimeIndex n: string specifying progress of queries to API (e.g., &#39;3 of 5&#39;, &#39;4 of 5&#39;, &#39;5 of 5&#39;) &quot;&quot;&quot; # still figuring out how to make Progress bar work in notebooks (this works fine in iTerm...) # bar = IncrementalBar(&#39;Query {0}: {1}&#39;.format(n,&#39;, &#39;.join(dims)), max=len(dates)) for i in dates: date_to_get = &#39;{0}&#39;.format(i.strftime(&#39;%Y-%m-%d&#39;)) time.sleep(1 + random.random()) # since we are downloading data day by day, start and end dates are the same # if you want a range of dates, skip this function and use execute_request() directly execute_request(dims,date_to_get,date_to_get) # bar.next() # bar.finish() . . Important: This will kickoff the process to authorize as well as build the service object with which you can interact with the API. It will print a URL, but should redirect you to that URL automagically. It&#8217;s more seemless if you are also logged in to the Google profile in your default browser that you want to authorize. . # you should only have to authorize on the first time running this line service = initialize_service(service = &#39;webmaster&#39;) . As mentioned above, you should automatically get redirected and see a screen like this. Since I am running Jupyter in regular http rather than https I got the scary red triangle and had to press the &quot;continue anyways&quot; button... . . Make sure you have the right list of APIs here: . . This will let you know you&#39;re done: . . Now to finally start getting data! First, set a time range you want to query. I run this script monthly and so use a lazy way of getting one month at a time. . # not strictly necessary to use pandas to convert to timestamp, but helpful to do so in order to get last day of month start_of_month = &#39;2020-01-01&#39; end_of_month = pd.to_datetime(start_of_month).to_period(&#39;M&#39;).to_timestamp(&#39;M&#39;) end_of_month . Timestamp(&#39;2020-01-31 00:00:00&#39;) . # convert start and end dates to date range using Pandas date_range = pd.date_range(start_of_month,end_of_month,freq=&#39;D&#39;,normalize=True) . Now that there&#39;s a date range to work with, query the API :smiley: . # now start getting the data and saving the JSON files # more example queries can be found here: https://developers.google.com/webmaster-tools/search-console-api-original/v3/how-tos/search_analytics # and there are a few more examples below execute_request([&quot;date&quot;],date_range.min().strftime(&#39;%Y-%m-%d&#39;),date_range.max().strftime(&#39;%Y-%m-%d&#39;)) . You could modify the execute_request function above to just return the JSON directly, but since I find myself running this script once a month to download the previous month&#39;s data, I have my own workflow where I just assume I&#39;m going to combine the output from all the previous months. . def read_json_files(path_to_file): with open(path_to_file) as p: data = json.load(p) return DataFrame.from_dict(data[0][&#39;rows&#39;]) . # i&#39;ve already download some other months of data besides january, which are shown here data_files = glob.glob(OUTPUT_PATH + &#39;*_date.json&#39;) data_files[:2] . [&#39;../output/search_console_data/2019-11-01_2019-11-30_date.json&#39;, &#39;../output/search_console_data/2020-01-01_2020-01-31_date.json&#39;] . # nice trick from a data science colleague to read in multiple files at once into one dataframe df = pd.concat([read_json_files(f) for f in data_files], ignore_index=True) df = df.drop([&#39;keys&#39;],axis=1) df.head() . clicks impressions ctr position date . 0 4.0 | 72.0 | 0.055556 | 28.930556 | 2019-11-01 | . 1 0.0 | 38.0 | 0.000000 | 40.263158 | 2019-11-02 | . 2 0.0 | 41.0 | 0.000000 | 39.390244 | 2019-11-03 | . 3 2.0 | 69.0 | 0.028986 | 24.898551 | 2019-11-04 | . 4 5.0 | 90.0 | 0.055556 | 30.466667 | 2019-11-05 | . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;],format=&#39;%Y-%m-%d&#39;) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 123 entries, 0 to 122 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 clicks 123 non-null float64 1 impressions 123 non-null float64 2 ctr 123 non-null float64 3 position 123 non-null float64 4 date 123 non-null datetime64[ns] dtypes: datetime64[ns](1), float64(4) memory usage: 4.9 KB . df.set_index(&#39;date&#39;)[&#39;clicks&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x119e0d280&gt; . df.groupby([pd.Grouper(key=&#39;date&#39;,freq=&#39;MS&#39;)])[&#39;clicks&#39;].sum().plot(marker=&#39;o&#39;,ylim=0); . Here are some more examples of the type of queries I run. Generally, the more granular the less data you get; so I run through a few permutations just to get as much info as I can. With a small blog like this, it doesn&#39;t really matter, whereas at work millions of visitors a month can really add up to a lot of rows here... . get_search_data([&quot;date&quot;,&quot;query&quot;,&quot;page&quot;],date_range,&#39;1 of 2&#39;) get_search_data([&quot;date&quot;,&quot;query&quot;,&quot;page&quot;,&quot;device&quot;,&quot;country&quot;],date_range,&#39;2 of 2&#39;) .",
            "url": "http://blog.measureallthethin.gs/apis/seo/pandas/jupyter/2020/05/17/google-search-console-via-python.html",
            "relUrl": "/apis/seo/pandas/jupyter/2020/05/17/google-search-console-via-python.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Finding Similar Time-Series",
            "content": "Ever since watching this presentation about quasi-experiments at Netflix I&#39;ve been trying to understand more about how to operationalize a similar model of testing. In my opinion, I&#39;ve found multi-channel attribution to be difficult because there are only so many insights I can believe. For example, finding a campaign that consistently results in $0 of revenue is probably accurate; when I&#39;ve dug into issues like that further, et voilà, I&#39;ve find ad fraud. On the other hand, an attribution tool that promises insights into the return from paid social media traffic (which I&#39;ve typically seen to be &gt;80% smartphone traffic) seems dubious. While quasi-experiments aren&#39;t perfect, I find the premise more promising than tying together a bunch of cookies -- and much, much better than utilizing the sketchy companies that have resolved cookies/email addresses/devices to a person. There are also some really cool use cases for this type of approach, like Airbnb experimenting with UX/SEO optimization. . While there&#39;s a lot of details that go into setting up infrastructure to support near-experimental design with marketing platforms, one of the starting points is finding which geographic regions are most similar so they can serve as control/test groups. One method that comes up oftern is dynamic time warping, a great example of which is the MarketMatching R package : . The MarketMatching package implements the workflow described above by essentially providing an easy-to-use &quot;wrapper&quot; for the dtw and CausalImpact. The function best_matches() finds the best control markets for each market by looping through all viable candidates in a parallel fashion and then ranking by distance and/or correlation. The resulting output object can then be passed to the inference() function which then analyzes the causal impact of an event using the pre-screened control markets. . If you&#39;d like to learn more about dynamic time warping and classification problems for time-series, see Aileen Nielsen&#39;s book Practical Time Series Analysis. Now on to the code. Of course, to make things more difficult for myself I want to try this out in Python... . %matplotlib inline import numpy as np from numpy.random import randn import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm from statsmodels.tsa.arima_process import arma_generate_sample np.random.seed(12345) . arparams = np.array([.75, -.25]) maparams = np.array([.65, .35]) ar = np.r_[1, -arparams] # add zero-lag and negate ma = np.r_[1, maparams] # add zero-lag y = sm.tsa.arma_generate_sample(ar, ma, nsample=250, distrvs=randn) # model = sm.tsa.ARMA(y, (2, 2)).fit(trend=&#39;nc&#39;, disp=0) # model.params fig, ax = plt.subplots() ax.plot(y, label=&quot;Data&quot;) . [&lt;matplotlib.lines.Line2D at 0x1c1b956828&gt;] . fig, ax = plt.subplots() ax.plot(randn(250).cumsum(), &#39;k&#39;, label=&#39;one&#39;) . [&lt;matplotlib.lines.Line2D at 0x1c1b62cda0&gt;] . from scipy.spatial.distance import euclidean from fastdtw import fastdtw .",
            "url": "http://blog.measureallthethin.gs/viz/jupyter/2020/05/07/time-series-and-time-warp.html",
            "relUrl": "/viz/jupyter/2020/05/07/time-series-and-time-warp.html",
            "date": " • May 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Plotting Fun With Matlotlib (and some Seaborn assistance)",
            "content": "Numerous times I&#39;ve found myself in a situation where data is not yet in a datawarehouse or available in a visualization tool (such as Looker), but nonetheless charts were expected for scheduled reports. This is a gallery of some of the charts I&#39;ve made. It&#39;s important to note that Chris Moffit&#39;s post on Effectively Using Matplotlib was instrumental to finally grokking how to interact with that charting library. . %matplotlib inline import numpy as np from numpy.random import randn import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set_style(&#39;whitegrid&#39;) sns.set_context(&quot;talk&quot;) np.random.seed(12345) . Create Some Fake Data . Most of the charts I deal with in Marketing Analytics / Business Operations have been based on timeseries data. In prepping for this post, I found simulating timeseries, especially the patterns I see frequently in marketing analytics, not as straightforward. Luckily a few different people on Twitter were helpful, especially this suggestion from Shahar Shani-Kadmiel: . pic.twitter.com/LYqJSLsZyx . &mdash; Shahar Shani-Kadmiel (@shaharkadmiel) May 5, 2020 . # set the time range time = pd.date_range( start = &#39;2018-01-01&#39;, end = &#39;2019-12-31&#39;, freq = &#39;D&#39; ) # leverage sine waves to create daily data with weekly patterns and a trend that shows growth over time amp = 1000 freq = 52 / 365 data = amp * np.sin(freq * 2 * np.pi * np.arange(time.size)) noise = 2 * np.random.rand(time.size) - 1 data += 0.2 * data.ptp() * noise trend = np.power(np.arange(time.size),1.25) data += trend data += 50000 data = np.around(data,decimals=0).astype(int) . def generate_series(time,amp,freq,power,vertical_shift): data = amp * np.sin(freq * 2 * np.pi * np.arange(time.size)) noise = 2 * np.random.rand(time.size) - 1 data += 0.2 * data.ptp() * noise trend = np.power(np.arange(time.size),power) data += trend data += vertical_shift data = np.around(data,decimals=0).astype(int) return data . date_range = pd.date_range(start = &#39;2018-01-01&#39;, end = &#39;2019-12-31&#39;,freq = &#39;D&#39;) . web_traffic = generate_series(date_range,10000,(52 / 365),1.45,50000) fig, ax = plt.subplots(figsize=(12,8)) ax.plot(date_range,web_traffic) . [&lt;matplotlib.lines.Line2D at 0x1c367906a0&gt;] . acct_create = generate_series(date_range,1000,(52 / 365),0.95,1000) fig, ax = plt.subplots(figsize=(12,8)) ax.plot(date_range,acct_create) . [&lt;matplotlib.lines.Line2D at 0x1c36d462b0&gt;] . traffic = pd.DataFrame(web_traffic,index=time,columns=[&#39;website_traffic&#39;]) traffic.index.name = &#39;date&#39; . accts = pd.DataFrame(acct_create,index=time,columns=[&#39;accounts_created&#39;]) accts.index.name = &#39;date&#39; . df = pd.concat([traffic,accts],axis=1) df = df.reset_index() df.head() . date website_traffic accounts_created . 0 2018-01-01 | 52765 | 976 | . 1 2018-01-02 | 56745 | 2065 | . 2 2018-01-03 | 61969 | 1675 | . 3 2018-01-04 | 54372 | 1212 | . 4 2018-01-05 | 42823 | 474 | . df.groupby(pd.Grouper(key=&#39;date&#39;,freq=&#39;MS&#39;)).sum().plot(subplots=True, layout=(1,2), figsize=(14, 6), sharey=False, ylim = 0); . df.groupby(pd.Grouper(key=&#39;date&#39;,freq=&#39;MS&#39;)).sum().assign(signup_rate = lambda x: x[&#39;accounts_created&#39;] / x[&#39;website_traffic&#39;]).loc[:,&#39;signup_rate&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c37b0a358&gt; .",
            "url": "http://blog.measureallthethin.gs/viz/jupyter/2020/05/05/plotting-with-matplotlib.html",
            "relUrl": "/viz/jupyter/2020/05/05/plotting-with-matplotlib.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Connect Python and Pandas To Redshift",
            "content": "There are many situations where you might like to get data from a database into a pandas dataframe. The simplest way to is to initialize a dataframe via the pandas read_sql_query method. The ORM of choice for pandas is SQLAlchemy. By installing a couple more packages, you can query Redshift data and read that into a dataframe with just a few lines of of Python code. . The Setup . This post assumes you have a number of Python packages already installed, such as pandas, numpy, sqlalchemy and iPython or Jupyter Lab. I&#39;ve used the Python distribution Anacoda, which can be downloaded here. It comes packed with many of the libraries you might need for data analysis. . The Dialects documentation for SQLAlchemy mentions that Redshift is supported through another Python package, which itself depends on a PostgreSQL driver. So, the additional packages needed for connecting to Redshift are redshift-sqlalchemy and psycopg2. If you already have Anaconda, you can install psycopg2 quickly using conda. For getting redshift-sqlalchemy installed, here are some docs on using conda &amp; pip together. That being said, the process of downloading the packages looked like this in my terminal: . conda install psycopg2 cd Downloads/redshift-sqlalchemy-0.4.1 python setup.py install . Of course, I did this in my default environment. Everything seems to work, but the Anaconda docs are very useful if you have multiple Python environments on your machine. . On To The Data . Start iPython in your terminal (or Notebook if you prefer). Note that the connection string follows the pattern &#39;flavor+DBAPI://username:password@host:port/database&#39;. Make sure to change the connection string below to your own! . import pandas as pd import numpy as np from sqlalchemy import create_engine # what is the database equivalent of &#39;hello world&#39; anyways? test_query = &quot;&quot;&quot; SELECT COUNT(*) FROM datawarehouse.users u WHERE u.date BETWEEN &#39;2015-01-01&#39; AND &#39;2015-05-31&#39; &quot;&quot;&quot; red_engine = create_engine(&#39;redshift+psycopg2://username:password@your.redshift.host.123456abcdef.us-east-1.redshift.amazonaws.com:port/database&#39;) test = pd.read_sql_query(test_query,red_engine) . If we look at the &quot;test&quot; object I see: . In [5]: test Out[5]: count 0 178545 . Of course, you&#39;ll need to modify the query string to match the settings for your own warehouse to test this out. . Update/Issues . It&#39;s been a while since I had to use this type of connection, so things may have changed since this was originally in June of 2015. Specifically, once my company required SSL connections, the setup above went south and I got errors like: . OperationalError: (OperationalError) sslmode value &quot;require&quot; invalid when SSL support is not compiled in None None . Samantha Zeitlin responded to my request for help with a super-thorough post to help debug the issue, which had to do with fixing symlinks. Unfortunately the Google Group where that response was posted is now defunct :weary: . Miraculously I happen to have the last post to that thread: . For posterity, a workaround that worked for me without having to rely on symlinks:&gt; &gt; As others have suggested, install with pip:&gt; $ `conda uninstall psycopg2`&gt; $ pip install psycopg2 . Python will throw an error when trying to import psycopg2, so run this:&gt; $ `export DYLD_FALLBACK_LIBRARY_PATH=$HOME/anaconda/lib/:$DYLD_FALLBACK_LIBRARY_PATH`&gt; &gt; Credit:http://stackoverflow.com/questions/27264574/import-psycopg2-library-not-loaded-libssl-1-0-0-dylib . If you have any issues or fixes, I&#39;d love to hear about them; you can reach out to me on Twitter. .",
            "url": "http://blog.measureallthethin.gs/redshift/pandas/2020/05/04/connecting-python-and-pandas-to-redshift.html",
            "relUrl": "/redshift/pandas/2020/05/04/connecting-python-and-pandas-to-redshift.html",
            "date": " • May 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Charting a Marketing Acquisition Funnel",
            "content": "%matplotlib inline import pandas as pd from collections import OrderedDict import matplotlib.pyplot as plt import matplotlib.patches as patches import seaborn as sns sns.set_style(&#39;whitegrid&#39;) . # start with some dummy data df = pd.DataFrame({ &#39;date&#39;:[&#39;2019-08-01&#39;,&#39;2019-09-01&#39;], &#39;website_traffic&#39;:[530771,558652], &#39;new_accounts&#39;:[15720,15900], &#39;account_verify&#39;:[12670,13884], &#39;product_activated&#39;:[10812,12909], &#39;paid_conversion&#39;:[654,908], &#39;some_other_metric&#39;:[13270,13678] }) df . date website_traffic new_accounts account_verify product_activated paid_conversion some_other_metric . 0 2019-08-01 | 530771 | 15720 | 12670 | 10812 | 654 | 13270 | . 1 2019-09-01 | 558652 | 15900 | 13884 | 12909 | 908 | 13678 | . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;],format=&#39;%Y-%m-%d&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2 entries, 0 to 1 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 date 2 non-null datetime64[ns] 1 website_traffic 2 non-null int64 2 new_accounts 2 non-null int64 3 account_verify 2 non-null int64 4 product_activated 2 non-null int64 5 paid_conversion 2 non-null int64 6 some_other_metric 2 non-null int64 dtypes: datetime64[ns](1), int64(6) memory usage: 240.0 bytes . # define only the KPIs we want to see (since there are cases where other metrics are present in the dataframe) all_kpis = [&#39;website_traffic&#39;,&#39;new_accounts&#39;,&#39;account_verify&#39;,&#39;product_activated&#39;,&#39;paid_conversion&#39;] . all_names = tuple([x.replace(&#39;_&#39;,&#39; n&#39;).title() for x in all_kpis]) . def get_it_in_order(df,mask,month): &quot;&quot;&quot; Biggest assumption: there is a &#39;date&#39; column (not index!) in the dataframe that is *literally named* &#39;date&#39; Accepts entire dataframe Supply a column mask Supply a list of column names &quot;&quot;&quot; kpis = OrderedDict() frame_2_dict = df.set_index(&#39;date&#39;).loc[month,mask].to_dict() for i in mask: kpis[i] = frame_2_dict[i] return kpis . def add_arrow(x_adjust,y_adjust,ratio): bbox_props = dict(boxstyle=&quot;rarrow,pad=0.5&quot;, facecolor=&quot;grey&quot;, edgecolor=&quot;grey&quot;, alpha=0.75, lw=1) return plt.text(x_adjust,y_adjust,&#39;{0:.1f}%&#39;.format(ratio * 100), verticalalignment=&#39;center&#39;, horizontalalignment=&#39;left&#39;, fontsize=16, weight=&#39;black&#39;, color=&#39;white&#39;, bbox=bbox_props) . def plot_acquisition_funnel(df, month, names, metrics): data = get_it_in_order(df, metrics, month) count = range(len(data)) fig, ax = plt.subplots(figsize=(10, 6)) rects = ax.bar(count, data.values(), width=.5, align=&#39;center&#39;, color=sns.color_palette(&quot;Blues&quot;, len(data))) plt.xticks(count, names) plt.ylim(bottom=0, top=data[metrics[1]]*2) plt.title(&#39;{0} Acquisition Funnel&#39;.format(month), fontsize=16) plt.tick_params(labelsize=16) add_arrow(0.27, data[metrics[3]]/1.7, data[metrics[1]]/data[metrics[0]]) add_arrow(1.27, data[metrics[3]]/1.7, data[metrics[2]]/data[metrics[1]]) add_arrow(2.27, data[metrics[3]]/1.7, data[metrics[3]]/data[metrics[2]]) add_arrow(3.27, data[metrics[3]]/1.7, data[metrics[4]]/data[metrics[3]]) for rect, val in zip(rects,data.values()): height = rect.get_height() if height &gt; 250: label_color = &#39;grey&#39; else: label_color = &#39;white&#39; ax.text(rect.get_x() + rect.get_width()/2,250,&#39;{:,}&#39;.format(val),ha=&#39;center&#39;, va=&#39;bottom&#39;,fontsize=12,weight=&#39;bold&#39;,color=label_color) plt.grid(False) plt.show() . plot_acquisition_funnel(df,&#39;2019-08-01&#39;,all_names,all_kpis) .",
            "url": "http://blog.measureallthethin.gs/viz/jupyter/2020/05/02/charting-a-marketing-funnel.html",
            "relUrl": "/viz/jupyter/2020/05/02/charting-a-marketing-funnel.html",
            "date": " • May 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "http://blog.measureallthethin.gs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.measureallthethin.gs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}